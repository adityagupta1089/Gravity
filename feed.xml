<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aditya Gupta</title>
    <description></description>
    <link>https://adityagupta1089.github.io/</link>
    <atom:link href="https://adityagupta1089.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 21 Oct 2019 11:44:59 +0000</pubDate>
    <lastBuildDate>Mon, 21 Oct 2019 11:44:59 +0000</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Interval Trees</title>
        <description>&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;Given a set of intervals $I$ on the real line and a query point $q_x$, find the intervals that contain $q_x$.&lt;/p&gt;

&lt;p&gt;Let $I:={[x_1:x_1’],[x_2:x_2’],\ldots,[x_n:x_n’]}$. Let $x_{\rm mid}$ be the median of the $2n$ interval endpoints such that atmost half of them lie on the left and half of them lie on the right.&lt;/p&gt;

&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/intervaltree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If $I=\phi$ then interval tree is a leaf.&lt;/li&gt;
  &lt;li&gt;Otherwise let $x_{\rm mid}$ be the median of the endpoints of the interval. Let
    &lt;ul&gt;
      &lt;li&gt;$I_{\rm left}:={[x_j:x_j’]\in I\mid x_j’&amp;lt;x_{\rm mid}}$&lt;/li&gt;
      &lt;li&gt;$I_{\rm mid}:={[x_j:x_j’]\in I\mid x_j\le x_{\rm mid}\le x_j’}$.&lt;/li&gt;
      &lt;li&gt;$I_{\rm right}:={[x_j:x_j’]\in I\mid x_j&amp;gt;x_{\rm mid}}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/intervaltree2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Interval tree consists of a root node $v$ storing $x_{\rm mid}$.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$I_{\rm mid}$ is stored twice:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;${\cal L}_{\rm left}$ that is sorted on the left endpoints of $I_{\rm mid}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;${\cal L}_{\rm right}$ that is sorted on the right endpoints of  $I_{\rm mid}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Left subtree of $v$ is an interval tree for the set $I_{\rm left}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Right subtree of $v$ is an interval tree for the set $I_{\rm right}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;creation-onlog-n&quot;&gt;Creation $O(n\log n)$&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;create(I)
  if I = null
    return empty leaf
  else
    create a node v
    computer x_mid (linear time, constant if presorted)
    store x_mid with v
    compute I_mid, L_left, L_right
    left_child(v) &amp;lt;- create(I_left)
    right_child(v) &amp;lt;- create(I_right)
    return v
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;querying-olog-nk&quot;&gt;Querying $O(\log n+k)$&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;If $q_x &amp;lt; x_{\rm mid}(v)$ walk along $\cal L_{\rm left}$ reporting all intervals that contain $q_x$. Stop as soon as an interval doesn’t contain $q_x$. Query left subtree of $v$.&lt;/li&gt;
  &lt;li&gt;Symmetrically do for $q_x&amp;gt;x_{\rm mid(v)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$k$ = Number of reported intervals.&lt;/p&gt;

&lt;h1 id=&quot;2-d-range-tree&quot;&gt;2-D Range Tree&lt;/h1&gt;

&lt;p&gt;The data structure to store a a horizontal line segments is $\rm T$. If we want to create a 2-D range tree then instead of $\cal L_{\rm left}(v)$ and $\cal L_{\rm right}(v)$ we will store:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A range tree $T_{\rm left}(v)$ on left endpoint segments in $I_{\rm mid}(v)$.&lt;/li&gt;
  &lt;li&gt;A range tree $T_{\rm right}(v)$ on right endpoint segments in $I_{\rm mid}(v)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At time of querying instead of walking along $\cal L_{\rm left}(v)$ and $\cal L_{\rm right}(v)$ we will perform a query in the range trees $\rm T_{left}(v)$ and $\rm T_{right}(v)$.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creation: $O(n\log n)$&lt;/li&gt;
  &lt;li&gt;Querying: $O(\log^2n+k)$&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 20 Oct 2019 13:07:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/interval-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/interval-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Red Black Trees</title>
        <description>&lt;h1 id=&quot;properties&quot;&gt;Properties&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Red/Black node&lt;/li&gt;
  &lt;li&gt;Root and leaf black&lt;/li&gt;
  &lt;li&gt;Children of red nodes are both black&lt;/li&gt;
  &lt;li&gt;All paths to leaves contains same nubmer of black nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Same as 2-3-4 tree (t=2) B-Tree.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Node with single value is same as a red node&lt;/li&gt;
  &lt;li&gt;Node with two values is same as a red node with a black child.&lt;/li&gt;
  &lt;li&gt;Node with three values is same as a red node with 2 black children.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 20 Oct 2019 12:59:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/data%20structures/notes/red-black-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/data%20structures/notes/red-black-trees.html</guid>
        
        
        <category>Data Structures</category>
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>B-Trees</title>
        <description>&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/btree1.png&quot; alt=&quot;B-Tree of height 3 containing minimum number of keys&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every node $x$ has $x.n$ keys and $x.n+1$ children (possibly undefined).&lt;/li&gt;
  &lt;li&gt;The key $x.k_i$ separates the range of keys stores in children subtrees, say $l_i$ then $l_1\le x.k_1\le l_2\le x.k_2\le \ldots x.k_n \le l_{n+1}$&lt;/li&gt;
  &lt;li&gt;All leaves have same depth&lt;/li&gt;
  &lt;li&gt;Each node except root has $t-1$ keys to $2t-1$ keys. Therefore $t$ to $2t$ children.&lt;/li&gt;
  &lt;li&gt;Height of tree $h=O(\log_tn)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;operations&quot;&gt;Operations&lt;/h1&gt;

&lt;h2 id=&quot;search-othotlog_tn&quot;&gt;Search $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the subtree where given key exists using linear search&lt;/li&gt;
  &lt;li&gt;recurse in that tree&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;create-o1&quot;&gt;Create $O(1)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Create a empty root node&lt;/li&gt;
  &lt;li&gt;Insert keys&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;insert-othotlog_tn&quot;&gt;Insert $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the the leaf node by searching.&lt;/li&gt;
  &lt;li&gt;If node is full split on median key to make two children nodes and insert the meadian key into parent node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For single pass, split whenever you come across a full node while searching so as to be assured to no more split any nodes while shifting the median key upwards.&lt;/p&gt;

&lt;h2 id=&quot;splitting&quot;&gt;Splitting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/btree2.png&quot; alt=&quot;Splitting a node with $t=4$&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make two nodes with keys less than and more than the median key respectively.&lt;/li&gt;
  &lt;li&gt;Move the children between the keys to respective trees.&lt;/li&gt;
  &lt;li&gt;Move the median key to parent node.&lt;/li&gt;
  &lt;li&gt;Move the children around the median key to the left and right new nodes respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;delete-othotlog_tn&quot;&gt;Delete $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;If key $k$ is in leaf node $x$ then delete $k$ from $x$.&lt;/li&gt;
  &lt;li&gt;If key $k$ is in internal node $x$ then
    &lt;ul&gt;
      &lt;li&gt;If left child $y$ of $k$ has $\ge t$ keys then find predecessor $k’$ of $k$ in $y$. Delete $k’$ from $y$ and replace $k$ by $k’$ in $x$.&lt;/li&gt;
      &lt;li&gt;Otherwise check for right child $z$ and do symmetrically.&lt;/li&gt;
      &lt;li&gt;Otherwise merge $y$ and $z$ along with $k$ into $y$. Recursively delete $k$ from $y$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:33:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Decision Trees</title>
        <description>&lt;h1 id=&quot;entropy-2-class&quot;&gt;Entropy 2-class&lt;/h1&gt;

&lt;p&gt;If $\cal I$ is the set of training examples, $p_+$ ratio of posititive examples in $\cal I$ and $p_-$ negative examples then entropy which measures impurity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Entropy}(I) = -p_+\log_2p_+ - p_-\log _2p_-&lt;/script&gt;

&lt;p&gt;This represents expected number of bits to encode the class of a randomly drawn member of $\cal I$&lt;/p&gt;

&lt;h1 id=&quot;information-gain&quot;&gt;Information Gain&lt;/h1&gt;

&lt;p&gt;Expected reduction in entropy due to sorting on attribute $x_i$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Gain}({\cal I}, x_i)=\text{Entropy}(I)-\sum_{v\in\text{values}(x
_i)}\frac{|\cal I_v|}{|\cal I|}\text{Entropy}(\cal I_v)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{values}(x_i)$: all possible values of $x_i$&lt;/li&gt;
  &lt;li&gt;$\cal I_v\subset \cal I$ : points in $\cal I$ that take value $v$ for $x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mutual-information-gain--information-gain&quot;&gt;Mutual Information Gain &amp;amp; Information Gain&lt;/h1&gt;

&lt;p&gt;Mutual information is the measure of mutual dependence of two random variables.&lt;/p&gt;

&lt;p&gt;Mutual information gain for two random variables $X$ and $Y$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=\sum_{y\in Y}\sum_{x\in X}p(x, y)\log\left(\frac{p(x,y)}{p(x)p(y)}\right)&lt;/script&gt;

&lt;p&gt;In decision trees mutual information gain and information gain are synonymous:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=H(Y)-H(Y|X)&lt;/script&gt;

&lt;h1 id=&quot;id3-algorithm&quot;&gt;ID3 Algorithm&lt;/h1&gt;

&lt;p&gt;Recursively perform on all examples.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For all $+$/$-$ examples return single node with corresponding label.&lt;/li&gt;
  &lt;li&gt;Otherwise split on attribute that best classifies current set of examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;decision-trees-features&quot;&gt;Decision Trees Features&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis space is complete&lt;/li&gt;
  &lt;li&gt;Only single hypothesis as output&lt;/li&gt;
  &lt;li&gt;No backtracking&lt;/li&gt;
  &lt;li&gt;Statistically based choices&lt;/li&gt;
  &lt;li&gt;True bias hard to estimate&lt;/li&gt;
  &lt;li&gt;Shorter trees are preferred&lt;/li&gt;
  &lt;li&gt;Trees with high information gain near root are preferred&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;overfitting&quot;&gt;Overfitting&lt;/h1&gt;

&lt;p&gt;Hypothesis $h\in \cal H$ overfits $X$ if $\exists h’$ such that:&lt;/p&gt;

&lt;p&gt;$E(h|X)&amp;lt; E(h’|X)$ but $E_p(h)&amp;gt;E_p(h’)$ where $p$ is the entire distribution of data.&lt;/p&gt;

&lt;p&gt;Strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stop when insignificant information gain&lt;/li&gt;
  &lt;li&gt;post pruning  - subtrees/rules/&lt;/li&gt;
  &lt;li&gt;using ensembles&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Real values functions - sort and split on midpoints&lt;/li&gt;
  &lt;li&gt;Alternative measures for selecting attributes - gain ratio / cost sensitive information gain&lt;/li&gt;
  &lt;li&gt;Missing attribute values - most commomn value at node / node with same target value / probability based on descendents of node&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;advantages--disadvantages&quot;&gt;Advantages / Disadvantages&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Easy to explain&lt;/li&gt;
  &lt;li&gt;No hyperparameters&lt;/li&gt;
  &lt;li&gt;Overfitting&lt;/li&gt;
  &lt;li&gt;Low accuracies (cf. other approaches)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;random-forests&quot;&gt;Random Forests&lt;/h1&gt;

&lt;h2 id=&quot;instane-bagging--bootstrap-aggregation&quot;&gt;Instane Bagging / Bootstrap aggregation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Generate $K$ different bootstrapped trainig datasets (sampling with replacement)&lt;/li&gt;
  &lt;li&gt;Learn a decision tree on each&lt;/li&gt;
  &lt;li&gt;Final prediction is the majority/average vote of all.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;freature-bagging&quot;&gt;Freature bagging&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Improved than instance bagging&lt;/li&gt;
  &lt;li&gt;Build a decision tree using a subset of $m$ features rather than all $D$ features, typically $m\approx \sqrt D$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:29:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Supervised Learning</title>
        <description>&lt;p&gt;Given a set $(x,y)$ need to estimate $f(x)=y$ .&lt;/p&gt;

&lt;p&gt;Terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature $x_i$ - property of object to be classified&lt;/li&gt;
  &lt;li&gt;Instance $x=\begin{pmatrix}x_1&amp;amp;x_2&amp;amp;x_3&amp;amp;\ldots\end{pmatrix}$ - features of an object&lt;/li&gt;
  &lt;li&gt;Instance Space $\cal I$ - space of all possible instances&lt;/li&gt;
  &lt;li&gt;Class $\cal Y$ - categorical feature of an object&lt;/li&gt;
  &lt;li&gt;Example $(x,y)$ - instance with membership&lt;/li&gt;
  &lt;li&gt;Training Set $X = {}_{i=1}^N{x_i,y_i}$&lt;/li&gt;
  &lt;li&gt;Target Concept $\cal C$ - correct expression of class.&lt;/li&gt;
  &lt;li&gt;Concept Class - Space of all possible concepts&lt;/li&gt;
  &lt;li&gt;Hypothesis $h :x\mapsto{0,1}$ - Approximation to target concept&lt;/li&gt;
  &lt;li&gt;Hypothesis class $\cal H$ - Space of all possible hypothesis&lt;/li&gt;
  &lt;li&gt;Learning Goal - Find $h\in\cal H$ that closely approximates $\cal C$ (possible $\cal C\not\in H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Errors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Empirical: $E(h|X) = \frac 1N\sum_{t=1}^N 1(h(x_t)\ne y_t)$&lt;/li&gt;
  &lt;li&gt;Generalization: instances not in $X$&lt;/li&gt;
  &lt;li&gt;True: instances in $\cal I$&lt;/li&gt;
  &lt;li&gt;Most specific and general hypothesis $\cal S$ and $\cal G$ covering fewest and most instances.&lt;/li&gt;
  &lt;li&gt;Version space - between $\cal S$ and $\cal G$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</guid>
        
        
        <category>Notes</category>
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Elastic Search Notes</title>
        <description>&lt;p&gt;These are the notes on Elastic Search which I wrote while studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/9e9af730f16384f54f53a1827cf1215e.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:08:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/elasticsearch-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/elasticsearch-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Haskell Notes</title>
        <description>&lt;p&gt;These are the notes on Haskell which I wrote when I was studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/0b7b7d332d37a1b1861bb79b76ee2b48.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:07:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/haskell-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/haskell-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Git Commands</title>
        <description>&lt;p&gt;Some common git commands frequently used&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/3ce91dbff58b2e56af6fcf01cfe98191.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:03:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/git-commands.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/git-commands.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Tutorial 6</title>
        <description>&lt;ol&gt;
  &lt;li&gt;We know that $P(A)\ge 0$, so we assume the opposite, i.e. $P(X\ge \mu)=0$ so $X&amp;lt;\mu$ with 100% certainity, we get $\mu=\sum_{x}xp(x)=\sum_{x&amp;lt; \mu}xp(x)$ and $\displaystyle\sum_{x&amp;lt;\mu}xp(x)&amp;lt;\sum_{x&amp;lt;\mu}\mu p(x)=\mu\sum_{x&amp;lt;\mu}p(x)=\mu P(X&amp;lt;\mu)=\mu$, a contradiction $\mu&amp;lt;\mu$. Similarly for the opposite case [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Lemma 6.2&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.3&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO (SAT not taught yet) [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.4&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 26}_{1,2}\cdot 2+\underbrace{\frac 16}_{5}\cdot 10=2.\bar 3&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 1{13}}_{\text K}\cdot 4+\underbrace{\frac 2{13}}_{5,10}\cdot 3+\underbrace{\frac 3{13}}_{3,6,9}\cdot 2\approx 1.230&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\left(\frac{2\cdot4}{36}\right)}_{(1,3),(2,4),(3,5),(4,6)}\times 2+\underbrace{\left(\frac{2\cdot1}{36}\right)}_{(1,6)}\times 5= 0.7\bar2&lt;/script&gt; Unfair.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 26 Sep 2017 07:25:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-6.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-6.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
      <item>
        <title>Tutorial 5</title>
        <description>&lt;ol&gt;
  &lt;li&gt;For a polynomial $p(x)$ of degree $d$, we had $p(x)=f(x)g(x)$, we concluded that $p(x)-f(x)g(x)$ is a polynomial of degree atmost $d$, hence can have atmost $d$ roots which are false-positives in this case. The probability that $n(&amp;gt;d)$ elements chosen out of &lt;script type=&quot;math/tex&quot;&gt;\{1,2,\ldots10^6d\}&lt;/script&gt; belong to these false-positives is
&lt;script type=&quot;math/tex&quot;&gt;\displaystyle\le\frac{\binom n d\binom {10^6d-d}{n-d}}{\binom {10^6d}n}=\frac{n!((10^6-1)d)!}{d!(n-d)!(n-d)!(10^6d-n)!}\frac{n!(10^6d-n)!}{(10^6d)!}&lt;/script&gt;&lt;br /&gt;
which becomes &lt;script type=&quot;math/tex&quot;&gt;\displaystyle\frac{n!^2((10^6-1)d)!}{d!(10^6d)!(n-d)!^2}=\frac{n^2(n-1)^2..1^2}{(n-d)^2(n-d-1)^2...1^2}\frac{1}{\binom {10^6d}d}\stackrel{n\to\infty}\longrightarrow\binom {10^6d}d^{-1}\\\displaystyle
=\frac{d(d-1)...1}{(10^6d)(10^6d-1)..(10^6d-(d-1))}\stackrel{d\to\infty}\longrightarrow \frac1{10^{6d}}&lt;/script&gt; which is astronomically small.&lt;/li&gt;
  &lt;li&gt;The determinant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_matrix&quot;&gt;Tutte Matrix&lt;/a&gt; which is a polynomial in $x_{ij}$’s $(i&amp;lt;j)$ is non-zero for a perfect-matching. Hence, we can use the polynomial verification algorithm to check if it is non-zero. Also see &lt;a href=&quot;http://www.imsc.res.in/~meena/matching/lecture7.pdf&quot;&gt;this&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Given $F:{\mathbb Z_n}\to{\mathbb Z_m}$ s.t. $F(x+y)=F(x)+F(y)$ which is a homomorphism. Given $n$ we can estimate $F(n)$ by writing $F(z)=F(k+(z-k))=F(k)+F(z-k)$ where the probability that this is right is $1-1/9=8/9$. If we can run this 5 times, that would mean trying different combinations of $k$ and $z-k$ and taking the majority which will be correct $1-\binom 53(1/9)^3(8/9)^2-\binom 54(1/9)^4(8/9)^1-\binom 55(1/9)^5(8/9)^0=19456/19683\approx98.8\%$&lt;/li&gt;
  &lt;li&gt;Probability of getting a min-cut was calculated in class as &lt;script type=&quot;math/tex&quot;&gt;\displaystyle\prod_{k=3}^n\left(1-\frac 2k\right)=\binom n2^{-1}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Probability of getting a min-cut after $\lambda$ operations: $1-\left(1-\binom n2^{-1}\right)^{\lambda}=1-\left(\frac{n(n-1)-2}{n(n-1)}\right)^{\lambda}\stackrel{\lambda \to\infty}\longrightarrow1$&lt;/li&gt;
  &lt;li&gt;$O(n!)$&lt;/li&gt;
  &lt;li&gt;TODO (Binary search on unsorted elements?)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 26 Sep 2017 07:20:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-5.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-5.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aditya Gupta</title>
    <description></description>
    <link>https://adityagupta1089.github.io/</link>
    <atom:link href="https://adityagupta1089.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 22 Oct 2019 17:27:22 +0000</pubDate>
    <lastBuildDate>Tue, 22 Oct 2019 17:27:22 +0000</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Range Trees</title>
        <description>&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;Given a set of $n$ points $P$ on a real line and a query interval $[x:x’]$ find all the points inside the interval.&lt;/p&gt;

&lt;p&gt;Construct a binary search tree from $P$ and perform the following query:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/rangetree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure: Example of searching in range tree&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Search for $x$ and $x’$ until we get to $v_{\rm split}$ where the search path splits.&lt;/li&gt;
  &lt;li&gt;From the left child of $v_{\rm split}$ we continue search with $x$ and at every node $v$ we where search path goes left we all points in right subtree of $v$.&lt;/li&gt;
  &lt;li&gt;Symmetrically we go right from $v_{\rm split}$ searching for $x’$ and taking left subtrees of $v$ respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Canonical Subset of node&lt;/strong&gt; $v$ (i.e. $P(v)$): subset of points stored in the leaves of the subtree at $v$.&lt;/p&gt;

&lt;h1 id=&quot;2-d-range-trees&quot;&gt;2-D Range Trees&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;The main tree is a balanced binary search tree built $T$ built on the x-coordinates of $P$.&lt;/li&gt;
  &lt;li&gt;For any internal node / leaf node $v$ in $T$, the canonical subset $P(v)$ is stored in a balanced binary search tree $T_1(v)$ on the y-coordinates of the points. The node $v$ contains a pointer to the root of $T_1(v)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;creation-onlog-n&quot;&gt;Creation $O(n\log n)$&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Use presorted $P$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create $T_1(v)$ binary search tree.&lt;/li&gt;
  &lt;li&gt;If $P$ has only one point then create leaf else split $P$ into two sets $P_{\rm left}$ and $P_{\rm right}$ using $x_{\rm mid}$ median point. Recursively create $v_{\rm left}$ and $v_{\rm right}$ from $P_{\rm left}$ and $P_{\rm right}$ respectively. Create a node with $x_{\rm mid}$ and $v_{\rm left}$ and $v_{\rm right}$ left and right children of this node. Make $T_1(v)$ the associated structure of $v$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;querying-olog-2n-k&quot;&gt;Querying $O(\log ^2n +k)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find $v_{\rm split}$&lt;/li&gt;
  &lt;li&gt;If $v_{\rm split}$ is a leaf check point inside it and report if necessary.&lt;/li&gt;
  &lt;li&gt;Else
    &lt;ul&gt;
      &lt;li&gt;Follow path to $x$ and perform 1-D range query on the subtrees right of the path. Also check if point stored at the final leaf node $v$ must be reported.&lt;/li&gt;
      &lt;li&gt;Similary do for $x’$ and perform 1-D range query on the subtrees left of the path from $v_{\rm split}$. Again check at the end for the point stored at $v$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fractional-cascading&quot;&gt;Fractional Cascading&lt;/h2&gt;

&lt;p&gt;If two sets of objects $S_1$ and $S_2$ are stored int sorted arrays $A_1$ and $A_2$. To find keys in $[y:y’]$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We can binary search for ceil of  $y$ in $A_1$ and then walk along the array until the floor of $y’$. Similary for $S_2$. Total time will be $O(k)$ plus two binary searches ($k$ reported objects).&lt;/li&gt;
  &lt;li&gt;If $S_2\subseteq S_1$ we can maintain pointers from $A_1$ to $A_2$, i.e. we store the pointer to ceil key for every value in $A_1$. This will avoid the second binary search.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/rangetree2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure: Layered Range Tree showing only x-coordinates. Full points are given below&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Similarly $P(lc(v))\subseteq P(v)$ and $P(rc(v))\subseteq P(v)$. Instead of associated binary trees we will store an array sorted on the y-coordinates. Each value in the array will additionaly store two pointers to $A(lc(v))$ and $A(rc(v))$. This becomes the &lt;strong&gt;layered range tree&lt;/strong&gt;. This makes the query time $O(\log n + k)$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/rangetree3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure: The associated arrays with the nodes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While querying for $[x:x’]\times[y:y’]$:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We search for $x$, $x’$ and $v_{\rm split}$. At $A(v_{\rm split})$ we we find the ceil entry of $y$.&lt;/li&gt;
  &lt;li&gt;While searching in $x$ and $x’$ in main tree we keep track of ceil entry of $y$ by following pointers in constant time.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 20 Oct 2019 15:54:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/range-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/range-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Interval Trees</title>
        <description>&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;Given a set of intervals $I$ on the real line and a query point $q_x$, find the intervals that contain $q_x$.&lt;/p&gt;

&lt;p&gt;Let $I:={[x_1:x_1’],[x_2:x_2’],\ldots,[x_n:x_n’]}$. Let $x_{\rm mid}$ be the median of the $2n$ interval endpoints such that atmost half of them lie on the left and half of them lie on the right.&lt;/p&gt;

&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/intervaltree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If $I=\phi$ then interval tree is a leaf.&lt;/li&gt;
  &lt;li&gt;Otherwise let $x_{\rm mid}$ be the median of the endpoints of the interval. Let
    &lt;ul&gt;
      &lt;li&gt;$I_{\rm left}:={[x_j:x_j’]\in I\mid x_j’&amp;lt;x_{\rm mid}}$&lt;/li&gt;
      &lt;li&gt;$I_{\rm mid}:={[x_j:x_j’]\in I\mid x_j\le x_{\rm mid}\le x_j’}$.&lt;/li&gt;
      &lt;li&gt;$I_{\rm right}:={[x_j:x_j’]\in I\mid x_j&amp;gt;x_{\rm mid}}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/intervaltree2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Interval tree consists of a root node $v$ storing $x_{\rm mid}$.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$I_{\rm mid}$ is stored twice:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;${\cal L}_{\rm left}$ that is sorted on the left endpoints of $I_{\rm mid}$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;${\cal L}_{\rm right}$ that is sorted on the right endpoints of  $I_{\rm mid}$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Left subtree of $v$ is an interval tree for the set $I_{\rm left}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Right subtree of $v$ is an interval tree for the set $I_{\rm right}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;creation-onlog-n&quot;&gt;Creation $O(n\log n)$&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&quot;language-pseudocode&quot;&gt;create(I)
  if I = null
    return empty leaf
  else
    create a node v
    computer x_mid (linear time, constant if presorted)
    store x_mid with v
    compute I_mid, L_left, L_right
    left_child(v) &amp;lt;- create(I_left)
    right_child(v) &amp;lt;- create(I_right)
    return v
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;querying-olog-nk&quot;&gt;Querying $O(\log n+k)$&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;If $q_x &amp;lt; x_{\rm mid}(v)$ walk along $\cal L_{\rm left}$ reporting all intervals that contain $q_x$. Stop as soon as an interval doesn’t contain $q_x$. Query left subtree of $v$.&lt;/li&gt;
  &lt;li&gt;Symmetrically do for $q_x&amp;gt;x_{\rm mid(v)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$k$ = Number of reported intervals.&lt;/p&gt;

&lt;h1 id=&quot;2-d-range-tree&quot;&gt;2-D Range Tree&lt;/h1&gt;

&lt;p&gt;The data structure to store a a horizontal line segments is $\rm T$. If we want to create a 2-D range tree then instead of $\cal L_{\rm left}(v)$ and $\cal L_{\rm right}(v)$ we will store:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A range tree $T_{\rm left}(v)$ on left endpoint segments in $I_{\rm mid}(v)$.&lt;/li&gt;
  &lt;li&gt;A range tree $T_{\rm right}(v)$ on right endpoint segments in $I_{\rm mid}(v)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At time of querying instead of walking along $\cal L_{\rm left}(v)$ and $\cal L_{\rm right}(v)$ we will perform a query in the range trees $\rm T_{left}(v)$ and $\rm T_{right}(v)$.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creation: $O(n\log n)$&lt;/li&gt;
  &lt;li&gt;Querying: $O(\log^2n+k)$&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 20 Oct 2019 13:07:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/interval-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/interval-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Red Black Trees</title>
        <description>&lt;h1 id=&quot;properties&quot;&gt;Properties&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Red/Black node&lt;/li&gt;
  &lt;li&gt;Root and leaf black&lt;/li&gt;
  &lt;li&gt;Children of red nodes are both black&lt;/li&gt;
  &lt;li&gt;All paths to leaves contains same nubmer of black nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Same as 2-3-4 tree (t=2) B-Tree.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Node with single value is same as a red node&lt;/li&gt;
  &lt;li&gt;Node with two values is same as a red node with a black child.&lt;/li&gt;
  &lt;li&gt;Node with three values is same as a red node with 2 black children.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 20 Oct 2019 12:59:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/data%20structures/notes/red-black-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/data%20structures/notes/red-black-trees.html</guid>
        
        
        <category>Data Structures</category>
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>B-Trees</title>
        <description>&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/btree1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure: B-Tree of height 3 containing minimum number of keys&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every node $x$ has $x.n$ keys and $x.n+1$ children (possibly undefined).&lt;/li&gt;
  &lt;li&gt;The key $x.k_i$ separates the range of keys stores in children subtrees, say $l_i$ then $l_1\le x.k_1\le l_2\le x.k_2\le \ldots x.k_n \le l_{n+1}$&lt;/li&gt;
  &lt;li&gt;All leaves have same depth&lt;/li&gt;
  &lt;li&gt;Each node except root has $t-1$ keys to $2t-1$ keys. Therefore $t$ to $2t$ children.&lt;/li&gt;
  &lt;li&gt;Height of tree $h=O(\log_tn)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;operations&quot;&gt;Operations&lt;/h1&gt;

&lt;h2 id=&quot;search-othotlog_tn&quot;&gt;Search $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the subtree where given key exists using linear search&lt;/li&gt;
  &lt;li&gt;recurse in that tree&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;create-o1&quot;&gt;Create $O(1)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Create a empty root node&lt;/li&gt;
  &lt;li&gt;Insert keys&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;insert-othotlog_tn&quot;&gt;Insert $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the the leaf node by searching.&lt;/li&gt;
  &lt;li&gt;If node is full split on median key to make two children nodes and insert the meadian key into parent node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For single pass, split whenever you come across a full node while searching so as to be assured to no more split any nodes while shifting the median key upwards.&lt;/p&gt;

&lt;h2 id=&quot;splitting&quot;&gt;Splitting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://adityagupta1089.github.io/images/btree2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure: Splitting a node with $t=4$&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make two nodes with keys less than and more than the median key respectively.&lt;/li&gt;
  &lt;li&gt;Move the children between the keys to respective trees.&lt;/li&gt;
  &lt;li&gt;Move the median key to parent node.&lt;/li&gt;
  &lt;li&gt;Move the children around the median key to the left and right new nodes respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;delete-othotlog_tn&quot;&gt;Delete $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;If key $k$ is in leaf node $x$ then delete $k$ from $x$.&lt;/li&gt;
  &lt;li&gt;If key $k$ is in internal node $x$ then
    &lt;ul&gt;
      &lt;li&gt;If left child $y$ of $k$ has $\ge t$ keys then find predecessor $k’$ of $k$ in $y$. Delete $k’$ from $y$ and replace $k$ by $k’$ in $x$.&lt;/li&gt;
      &lt;li&gt;Otherwise check for right child $z$ and do symmetrically.&lt;/li&gt;
      &lt;li&gt;Otherwise merge $y$ and $z$ along with $k$ into $y$. Recursively delete $k$ from $y$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:33:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Decision Trees</title>
        <description>&lt;h1 id=&quot;entropy-2-class&quot;&gt;Entropy 2-class&lt;/h1&gt;

&lt;p&gt;If $\cal I$ is the set of training examples, $p_+$ ratio of posititive examples in $\cal I$ and $p_-$ negative examples then entropy which measures impurity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Entropy}(I) = -p_+\log_2p_+ - p_-\log _2p_-&lt;/script&gt;

&lt;p&gt;This represents expected number of bits to encode the class of a randomly drawn member of $\cal I$&lt;/p&gt;

&lt;h1 id=&quot;information-gain&quot;&gt;Information Gain&lt;/h1&gt;

&lt;p&gt;Expected reduction in entropy due to sorting on attribute $x_i$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Gain}({\cal I}, x_i)=\text{Entropy}(I)-\sum_{v\in\text{values}(x
_i)}\frac{|\cal I_v|}{|\cal I|}\text{Entropy}(\cal I_v)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{values}(x_i)$: all possible values of $x_i$&lt;/li&gt;
  &lt;li&gt;$\cal I_v\subset \cal I$ : points in $\cal I$ that take value $v$ for $x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mutual-information-gain--information-gain&quot;&gt;Mutual Information Gain &amp;amp; Information Gain&lt;/h1&gt;

&lt;p&gt;Mutual information is the measure of mutual dependence of two random variables.&lt;/p&gt;

&lt;p&gt;Mutual information gain for two random variables $X$ and $Y$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=\sum_{y\in Y}\sum_{x\in X}p(x, y)\log\left(\frac{p(x,y)}{p(x)p(y)}\right)&lt;/script&gt;

&lt;p&gt;In decision trees mutual information gain and information gain are synonymous:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=H(Y)-H(Y|X)&lt;/script&gt;

&lt;h1 id=&quot;id3-algorithm&quot;&gt;ID3 Algorithm&lt;/h1&gt;

&lt;p&gt;Recursively perform on all examples.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For all $+$/$-$ examples return single node with corresponding label.&lt;/li&gt;
  &lt;li&gt;Otherwise split on attribute that best classifies current set of examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;decision-trees-features&quot;&gt;Decision Trees Features&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis space is complete&lt;/li&gt;
  &lt;li&gt;Only single hypothesis as output&lt;/li&gt;
  &lt;li&gt;No backtracking&lt;/li&gt;
  &lt;li&gt;Statistically based choices&lt;/li&gt;
  &lt;li&gt;True bias hard to estimate&lt;/li&gt;
  &lt;li&gt;Shorter trees are preferred&lt;/li&gt;
  &lt;li&gt;Trees with high information gain near root are preferred&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;overfitting&quot;&gt;Overfitting&lt;/h1&gt;

&lt;p&gt;Hypothesis $h\in \cal H$ overfits $X$ if $\exists h’$ such that:&lt;/p&gt;

&lt;p&gt;$E(h|X)&amp;lt; E(h’|X)$ but $E_p(h)&amp;gt;E_p(h’)$ where $p$ is the entire distribution of data.&lt;/p&gt;

&lt;p&gt;Strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stop when insignificant information gain&lt;/li&gt;
  &lt;li&gt;post pruning  - subtrees/rules/&lt;/li&gt;
  &lt;li&gt;using ensembles&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Real values functions - sort and split on midpoints&lt;/li&gt;
  &lt;li&gt;Alternative measures for selecting attributes - gain ratio / cost sensitive information gain&lt;/li&gt;
  &lt;li&gt;Missing attribute values - most commomn value at node / node with same target value / probability based on descendents of node&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;advantages--disadvantages&quot;&gt;Advantages / Disadvantages&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Easy to explain&lt;/li&gt;
  &lt;li&gt;No hyperparameters&lt;/li&gt;
  &lt;li&gt;Overfitting&lt;/li&gt;
  &lt;li&gt;Low accuracies (cf. other approaches)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;random-forests&quot;&gt;Random Forests&lt;/h1&gt;

&lt;h2 id=&quot;instane-bagging--bootstrap-aggregation&quot;&gt;Instane Bagging / Bootstrap aggregation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Generate $K$ different bootstrapped trainig datasets (sampling with replacement)&lt;/li&gt;
  &lt;li&gt;Learn a decision tree on each&lt;/li&gt;
  &lt;li&gt;Final prediction is the majority/average vote of all.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;freature-bagging&quot;&gt;Freature bagging&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Improved than instance bagging&lt;/li&gt;
  &lt;li&gt;Build a decision tree using a subset of $m$ features rather than all $D$ features, typically $m\approx \sqrt D$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:29:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Supervised Learning</title>
        <description>&lt;p&gt;Given a set $(x,y)$ need to estimate $f(x)=y$ .&lt;/p&gt;

&lt;p&gt;Terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature $x_i$ - property of object to be classified&lt;/li&gt;
  &lt;li&gt;Instance $x=\begin{pmatrix}x_1&amp;amp;x_2&amp;amp;x_3&amp;amp;\ldots\end{pmatrix}$ - features of an object&lt;/li&gt;
  &lt;li&gt;Instance Space $\cal I$ - space of all possible instances&lt;/li&gt;
  &lt;li&gt;Class $\cal Y$ - categorical feature of an object&lt;/li&gt;
  &lt;li&gt;Example $(x,y)$ - instance with membership&lt;/li&gt;
  &lt;li&gt;Training Set $X = {}_{i=1}^N{x_i,y_i}$&lt;/li&gt;
  &lt;li&gt;Target Concept $\cal C$ - correct expression of class.&lt;/li&gt;
  &lt;li&gt;Concept Class - Space of all possible concepts&lt;/li&gt;
  &lt;li&gt;Hypothesis $h :x\mapsto{0,1}$ - Approximation to target concept&lt;/li&gt;
  &lt;li&gt;Hypothesis class $\cal H$ - Space of all possible hypothesis&lt;/li&gt;
  &lt;li&gt;Learning Goal - Find $h\in\cal H$ that closely approximates $\cal C$ (possible $\cal C\not\in H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Errors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Empirical: $E(h|X) = \frac 1N\sum_{t=1}^N 1(h(x_t)\ne y_t)$&lt;/li&gt;
  &lt;li&gt;Generalization: instances not in $X$&lt;/li&gt;
  &lt;li&gt;True: instances in $\cal I$&lt;/li&gt;
  &lt;li&gt;Most specific and general hypothesis $\cal S$ and $\cal G$ covering fewest and most instances.&lt;/li&gt;
  &lt;li&gt;Version space - between $\cal S$ and $\cal G$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</guid>
        
        
        <category>Notes</category>
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Elastic Search Notes</title>
        <description>&lt;p&gt;These are the notes on Elastic Search which I wrote while studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/9e9af730f16384f54f53a1827cf1215e.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:08:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/elasticsearch-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/elasticsearch-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Haskell Notes</title>
        <description>&lt;p&gt;These are the notes on Haskell which I wrote when I was studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/0b7b7d332d37a1b1861bb79b76ee2b48.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:07:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/haskell-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/haskell-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Git Commands</title>
        <description>&lt;p&gt;Some common git commands frequently used&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/3ce91dbff58b2e56af6fcf01cfe98191.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:03:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/git-commands.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/git-commands.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Tutorial 6</title>
        <description>&lt;ol&gt;
  &lt;li&gt;We know that $P(A)\ge 0$, so we assume the opposite, i.e. $P(X\ge \mu)=0$ so $X&amp;lt;\mu$ with 100% certainity, we get $\mu=\sum_{x}xp(x)=\sum_{x&amp;lt; \mu}xp(x)$ and $\displaystyle\sum_{x&amp;lt;\mu}xp(x)&amp;lt;\sum_{x&amp;lt;\mu}\mu p(x)=\mu\sum_{x&amp;lt;\mu}p(x)=\mu P(X&amp;lt;\mu)=\mu$, a contradiction $\mu&amp;lt;\mu$. Similarly for the opposite case [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Lemma 6.2&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.3&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO (SAT not taught yet) [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.4&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 26}_{1,2}\cdot 2+\underbrace{\frac 16}_{5}\cdot 10=2.\bar 3&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 1{13}}_{\text K}\cdot 4+\underbrace{\frac 2{13}}_{5,10}\cdot 3+\underbrace{\frac 3{13}}_{3,6,9}\cdot 2\approx 1.230&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\left(\frac{2\cdot4}{36}\right)}_{(1,3),(2,4),(3,5),(4,6)}\times 2+\underbrace{\left(\frac{2\cdot1}{36}\right)}_{(1,6)}\times 5= 0.7\bar2&lt;/script&gt; Unfair.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 26 Sep 2017 07:25:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-6.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-6.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aditya Gupta</title>
    <description></description>
    <link>https://adityagupta1089.github.io/</link>
    <atom:link href="https://adityagupta1089.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 14 Oct 2019 10:25:57 +0000</pubDate>
    <lastBuildDate>Mon, 14 Oct 2019 10:25:57 +0000</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>B-Trees</title>
        <description>&lt;h1 id=&quot;definition&quot;&gt;Definition&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Every node $x$ has $x.n$ keys and $x.n+1$ children (possibly undefined).&lt;/li&gt;
  &lt;li&gt;The key $x.k_i$ separates the range of keys stores in children subtrees, say $l_i$ then $l_1\le x.k_1\le l_2\le x.k_2\le \ldots x.k_n \le l_{n+1}$&lt;/li&gt;
  &lt;li&gt;All leaves have same depth&lt;/li&gt;
  &lt;li&gt;Each node except root has $t-1$ keys to $2t-1$ keys. Therefore $t$ to $2t$ children.&lt;/li&gt;
  &lt;li&gt;Height of tree $h=O(\log_tn)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;operations&quot;&gt;Operations&lt;/h1&gt;

&lt;h2 id=&quot;search-othotlog_tn&quot;&gt;Search $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the subtree where given key exists using linear search&lt;/li&gt;
  &lt;li&gt;recurse in that tree&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;create-o1&quot;&gt;Create $O(1)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Create a empty root node&lt;/li&gt;
  &lt;li&gt;Insert keys&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;insert-othotlog_tn&quot;&gt;Insert $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find the the leaf node by searching.&lt;/li&gt;
  &lt;li&gt;If node is full split on median key to make two children nodes and insert the meadian key into parent node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For single pass, split whenever you come across a full node while searching so as to be assured to no more split any nodes while shifting the median key upwards.&lt;/p&gt;

&lt;h2 id=&quot;splitting&quot;&gt;Splitting&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Make two nodes with keys less than and more than the median key respectively.&lt;/li&gt;
  &lt;li&gt;Move the children between the keys to respective trees.&lt;/li&gt;
  &lt;li&gt;Move the median key to parent node.&lt;/li&gt;
  &lt;li&gt;Move the children around the median key to the left and right new nodes respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;delete-othotlog_tn&quot;&gt;Delete $O(th)=O(t\log_tn)$&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;If key $k$ is in leaf node $x$ then delete $k$ from $x$.&lt;/li&gt;
  &lt;li&gt;If key $k$ is in internal node $x$ then
    &lt;ul&gt;
      &lt;li&gt;If left child $y$ of $k$ has $\ge t$ keys then find predecessor $k’$ of $k$ in $y$. Delete $k’$ from $y$ and replace $k$ by $k’$ in $x$.&lt;/li&gt;
      &lt;li&gt;Otherwise check for right child $z$ and do symmetrically.&lt;/li&gt;
      &lt;li&gt;Otherwise merge $y$ and $z$ along with $k$ into $y$. Recursively delete $k$ from $y$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:33:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/b-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Decision Trees</title>
        <description>&lt;h1 id=&quot;entropy-2-class&quot;&gt;Entropy 2-class&lt;/h1&gt;

&lt;p&gt;If $\cal I$ is the set of training examples, $p_+$ ratio of posititive examples in $\cal I$ and $p_-$ negative examples then entropy which measures impurity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Entropy}(I) = -p_+\log_2p_+ - p_-\log _2p_-&lt;/script&gt;

&lt;p&gt;This represents expected number of bits to encode the class of a randomly drawn member of $\cal I$&lt;/p&gt;

&lt;h1 id=&quot;information-gain&quot;&gt;Information Gain&lt;/h1&gt;

&lt;p&gt;Expected reduction in entropy due to sorting on attribute $x_i$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Gain}({\cal I}, x_i)=\text{Entropy}(I)-\sum_{v\in\text{values}(x
_i)}\frac{|\cal I_v|}{|\cal I|}\text{Entropy}(\cal I_v)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{values}(x_i)$: all possible values of $x_i$&lt;/li&gt;
  &lt;li&gt;$\cal I_v\subset \cal I$ : points in $\cal I$ that take value $v$ for $x_i$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;mutual-information-gain--information-gain&quot;&gt;Mutual Information Gain &amp;amp; Information Gain&lt;/h1&gt;

&lt;p&gt;Mutual information is the measure of mutual dependence of two random variables.&lt;/p&gt;

&lt;p&gt;Mutual information gain for two random variables $X$ and $Y$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=\sum_{y\in Y}\sum_{x\in X}p(x, y)\log\left(\frac{p(x,y)}{p(x)p(y)}\right)&lt;/script&gt;

&lt;p&gt;In decision trees mutual information gain and information gain are synonymous:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(X;Y)=H(Y)-H(Y|X)&lt;/script&gt;

&lt;h1 id=&quot;id3-algorithm&quot;&gt;ID3 Algorithm&lt;/h1&gt;

&lt;p&gt;Recursively perform on all examples.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For all $+$/$-$ examples return single node with corresponding label.&lt;/li&gt;
  &lt;li&gt;Otherwise split on attribute that best classifies current set of examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;decision-trees-features&quot;&gt;Decision Trees Features&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis space is complete&lt;/li&gt;
  &lt;li&gt;Only single hypothesis as output&lt;/li&gt;
  &lt;li&gt;No backtracking&lt;/li&gt;
  &lt;li&gt;Statistically based choices&lt;/li&gt;
  &lt;li&gt;True bias hard to estimate&lt;/li&gt;
  &lt;li&gt;Shorter trees are preferred&lt;/li&gt;
  &lt;li&gt;Trees with high information gain near root are preferred&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;overfitting&quot;&gt;Overfitting&lt;/h1&gt;

&lt;p&gt;Hypothesis $h\in \cal H$ overfits $X$ if $\exists h’$ such that:&lt;/p&gt;

&lt;p&gt;$E(h|X)&amp;lt; E(h’|X)$ but $E_p(h)&amp;gt;E_p(h’)$ where $p$ is the entire distribution of data.&lt;/p&gt;

&lt;p&gt;Strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stop when insignificant information gain&lt;/li&gt;
  &lt;li&gt;post pruning  - subtrees/rules/&lt;/li&gt;
  &lt;li&gt;using ensembles&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Real values functions - sort and split on midpoints&lt;/li&gt;
  &lt;li&gt;Alternative measures for selecting attributes - gain ratio / cost sensitive information gain&lt;/li&gt;
  &lt;li&gt;Missing attribute values - most commomn value at node / node with same target value / probability based on descendents of node&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;advantages--disadvantages&quot;&gt;Advantages / Disadvantages&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Easy to explain&lt;/li&gt;
  &lt;li&gt;No hyperparameters&lt;/li&gt;
  &lt;li&gt;Overfitting&lt;/li&gt;
  &lt;li&gt;Low accuracies (cf. other approaches)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;random-forests&quot;&gt;Random Forests&lt;/h1&gt;

&lt;h2 id=&quot;instane-bagging--bootstrap-aggregation&quot;&gt;Instane Bagging / Bootstrap aggregation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Generate $K$ different bootstrapped trainig datasets (sampling with replacement)&lt;/li&gt;
  &lt;li&gt;Learn a decision tree on each&lt;/li&gt;
  &lt;li&gt;Final prediction is the majority/average vote of all.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;freature-bagging&quot;&gt;Freature bagging&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Improved than instance bagging&lt;/li&gt;
  &lt;li&gt;Build a decision tree using a subset of $m$ features rather than all $D$ features, typically $m\approx \sqrt D$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 09:29:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/data%20structures/decision-trees.html</guid>
        
        
        <category>Notes</category>
        
        <category>Data Structures</category>
        
      </item>
    
      <item>
        <title>Supervised Learning</title>
        <description>&lt;p&gt;Given a set $(x,y)$ need to estimate $f(x)=y$ .&lt;/p&gt;

&lt;p&gt;Terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature $x_i$ - property of object to be classified&lt;/li&gt;
  &lt;li&gt;Instance $x=\begin{pmatrix}x_1&amp;amp;x_2&amp;amp;x_3&amp;amp;\ldots\end{pmatrix}$ - features of an object&lt;/li&gt;
  &lt;li&gt;Instance Space $\cal I$ - space of all possible instances&lt;/li&gt;
  &lt;li&gt;Class $\cal Y$ - categorical feature of an object&lt;/li&gt;
  &lt;li&gt;Example $(x,y)$ - instance with membership&lt;/li&gt;
  &lt;li&gt;Training Set $X = {}_{i=1}^N{x_i,y_i}$&lt;/li&gt;
  &lt;li&gt;Target Concept $\cal C$ - correct expression of class.&lt;/li&gt;
  &lt;li&gt;Concept Class - Space of all possible concepts&lt;/li&gt;
  &lt;li&gt;Hypothesis $h :x\mapsto{0,1}$ - Approximation to target concept&lt;/li&gt;
  &lt;li&gt;Hypothesis class $\cal H$ - Space of all possible hypothesis&lt;/li&gt;
  &lt;li&gt;Learning Goal - Find $h\in\cal H$ that closely approximates $\cal C$ (possible $\cal C\not\in H$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Errors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Empirical: $E(h|X) = \frac 1N\sum_{t=1}^N 1(h(x_t)\ne y_t)$&lt;/li&gt;
  &lt;li&gt;Generalization: instances not in $X$&lt;/li&gt;
  &lt;li&gt;True: instances in $\cal I$&lt;/li&gt;
  &lt;li&gt;Most specific and general hypothesis $\cal S$ and $\cal G$ covering fewest and most instances.&lt;/li&gt;
  &lt;li&gt;Version space - between $\cal S$ and $\cal G$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/machine%20learning/Supervised-Learning.html</guid>
        
        
        <category>Notes</category>
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Elastic Search Notes</title>
        <description>&lt;p&gt;These are the notes on Elastic Search which I wrote while studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/9e9af730f16384f54f53a1827cf1215e.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:08:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/elasticsearch-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/elasticsearch-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Haskell Notes</title>
        <description>&lt;p&gt;These are the notes on Haskell which I wrote when I was studying it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/0b7b7d332d37a1b1861bb79b76ee2b48.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:07:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/haskell-notes.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/haskell-notes.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Git Commands</title>
        <description>&lt;p&gt;Some common git commands frequently used&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/adityagupta1089/3ce91dbff58b2e56af6fcf01cfe98191.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 07 Jun 2019 08:03:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/notes/git-commands.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/notes/git-commands.html</guid>
        
        
        <category>Notes</category>
        
      </item>
    
      <item>
        <title>Tutorial 6</title>
        <description>&lt;ol&gt;
  &lt;li&gt;We know that $P(A)\ge 0$, so we assume the opposite, i.e. $P(X\ge \mu)=0$ so $X&amp;lt;\mu$ with 100% certainity, we get $\mu=\sum_{x}xp(x)=\sum_{x&amp;lt; \mu}xp(x)$ and $\displaystyle\sum_{x&amp;lt;\mu}xp(x)&amp;lt;\sum_{x&amp;lt;\mu}\mu p(x)=\mu\sum_{x&amp;lt;\mu}p(x)=\mu P(X&amp;lt;\mu)=\mu$, a contradiction $\mu&amp;lt;\mu$. Similarly for the opposite case [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Lemma 6.2&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.3&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;TODO (SAT not taught yet) [&lt;strong&gt;Reference&lt;/strong&gt;: &lt;em&gt;Probability and Computing: Randomized Algorithms and Probabilistic Analysis&lt;/em&gt; by &lt;em&gt;Michael Mitzenmacher, Eli Upfal&lt;/em&gt; (&lt;strong&gt;Theorem 6.4&lt;/strong&gt;)]&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 26}_{1,2}\cdot 2+\underbrace{\frac 16}_{5}\cdot 10=2.\bar 3&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\frac 1{13}}_{\text K}\cdot 4+\underbrace{\frac 2{13}}_{5,10}\cdot 3+\underbrace{\frac 3{13}}_{3,6,9}\cdot 2\approx 1.230&lt;/script&gt; Unfair.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;E[M]=\underbrace{\left(\frac{2\cdot4}{36}\right)}_{(1,3),(2,4),(3,5),(4,6)}\times 2+\underbrace{\left(\frac{2\cdot1}{36}\right)}_{(1,6)}\times 5= 0.7\bar2&lt;/script&gt; Unfair.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 26 Sep 2017 07:25:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-6.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-6.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
      <item>
        <title>Tutorial 5</title>
        <description>&lt;ol&gt;
  &lt;li&gt;For a polynomial $p(x)$ of degree $d$, we had $p(x)=f(x)g(x)$, we concluded that $p(x)-f(x)g(x)$ is a polynomial of degree atmost $d$, hence can have atmost $d$ roots which are false-positives in this case. The probability that $n(&amp;gt;d)$ elements chosen out of &lt;script type=&quot;math/tex&quot;&gt;\{1,2,\ldots10^6d\}&lt;/script&gt; belong to these false-positives is
&lt;script type=&quot;math/tex&quot;&gt;\displaystyle\le\frac{\binom n d\binom {10^6d-d}{n-d}}{\binom {10^6d}n}=\frac{n!((10^6-1)d)!}{d!(n-d)!(n-d)!(10^6d-n)!}\frac{n!(10^6d-n)!}{(10^6d)!}&lt;/script&gt;&lt;br /&gt;
which becomes &lt;script type=&quot;math/tex&quot;&gt;\displaystyle\frac{n!^2((10^6-1)d)!}{d!(10^6d)!(n-d)!^2}=\frac{n^2(n-1)^2..1^2}{(n-d)^2(n-d-1)^2...1^2}\frac{1}{\binom {10^6d}d}\stackrel{n\to\infty}\longrightarrow\binom {10^6d}d^{-1}\\\displaystyle
=\frac{d(d-1)...1}{(10^6d)(10^6d-1)..(10^6d-(d-1))}\stackrel{d\to\infty}\longrightarrow \frac1{10^{6d}}&lt;/script&gt; which is astronomically small.&lt;/li&gt;
  &lt;li&gt;The determinant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_matrix&quot;&gt;Tutte Matrix&lt;/a&gt; which is a polynomial in $x_{ij}$’s $(i&amp;lt;j)$ is non-zero for a perfect-matching. Hence, we can use the polynomial verification algorithm to check if it is non-zero. Also see &lt;a href=&quot;http://www.imsc.res.in/~meena/matching/lecture7.pdf&quot;&gt;this&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Given $F:{\mathbb Z_n}\to{\mathbb Z_m}$ s.t. $F(x+y)=F(x)+F(y)$ which is a homomorphism. Given $n$ we can estimate $F(n)$ by writing $F(z)=F(k+(z-k))=F(k)+F(z-k)$ where the probability that this is right is $1-1/9=8/9$. If we can run this 5 times, that would mean trying different combinations of $k$ and $z-k$ and taking the majority which will be correct $1-\binom 53(1/9)^3(8/9)^2-\binom 54(1/9)^4(8/9)^1-\binom 55(1/9)^5(8/9)^0=19456/19683\approx98.8\%$&lt;/li&gt;
  &lt;li&gt;Probability of getting a min-cut was calculated in class as &lt;script type=&quot;math/tex&quot;&gt;\displaystyle\prod_{k=3}^n\left(1-\frac 2k\right)=\binom n2^{-1}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Probability of getting a min-cut after $\lambda$ operations: $1-\left(1-\binom n2^{-1}\right)^{\lambda}=1-\left(\frac{n(n-1)-2}{n(n-1)}\right)^{\lambda}\stackrel{\lambda \to\infty}\longrightarrow1$&lt;/li&gt;
  &lt;li&gt;$O(n!)$&lt;/li&gt;
  &lt;li&gt;TODO (Binary search on unsorted elements?)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 26 Sep 2017 07:20:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-5.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-5.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
      <item>
        <title>Tutorial 4</title>
        <description>&lt;ol&gt;
  &lt;li&gt;Given a binary string, the total strings that have maximum length of zeroes as $k$ can be found out by defining $a(n;k)$ as the valid strings that end at $1$ and $b(n;k)$ as the valid strings that end at $0$. Then we have:&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;We can $1$ to end of any valid string:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;a(n;k)=a(n-1;k)+b(n-1;k)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;We can add upto $k$ consecutive zeroes after a $1$:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;b(n;k)=a(n-1;k)+a(n-2;k)+...a(n-k;k)&lt;/script&gt;&lt;br /&gt;
Which can be written as:
&lt;script type=&quot;math/tex&quot;&gt;a(n-1;k)+a(n-2;k)+..a(n-k;k)=b(n;k)=a(n+1;k)-a(n;k)\\\displaystyle
\implies a(n+1;k)=\sum_{i=n-k}^na(i;k)&lt;/script&gt;
Solve using generating functions, Define $T(n;k)=a(n;k)+b(n;k)$ and then the expected value is $\frac 1{2^n}\sum T(n;k)\cdot k$ (&lt;strong&gt;Note&lt;/strong&gt;: Calculation remaining.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Suppose $ai\equiv aj\pmod n\implies a(i-j)\equiv 0\pmod n\implies i-j\equiv 0\pmod n$ (since $(a,n)=1$) hence $i=j$&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle&quot;&gt;Fisher-Yates Shuffle&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;We check $AB=C$ by finding $ABv$ and $Cv$ which is computationally faster. Suppose $AB= C$ then $\forall v\in{\mathbb Z}_k^n$, $ABv=Cv$. However, if $AB\ne C\implies (AB-C)\ne {\bf 0}$ then we may find some vectors $v\in {\mathbb Z}_k^n$ such that $(AB-C)v={\bf 0}$. But for every $u$ such that $(AB-C)u\ne {\bf 0}$ (whose existence is easy to prove by inspecting the entries of $AB-C$) we have $(AB-C)(u+v)\ne {\bf 0}$, $(AB-C)(u+2v)\ne {\bf 0}$, till $(AB-C)(u+(k-1)v)\ne {\bf 0}$ hence&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\displaystyle |\{u\in {\mathbb Z}_k^n|(AB-C)u= {\bf 0}\}|\le\frac 1{k}|{\mathbb Z_k^n}|=k^{n-1}&lt;/script&gt;&lt;br /&gt;
Hence the probability with which we get the correct result is $1-1/{k^t}$ where we perform $t$ iterations. In the case $k\to\infty$, that becomes close to $1$.&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{52}\left(\frac 1{52}\cdot 1 +\frac {51}{52}\cdot 0\right)=1&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{52}\left(\frac 1{52-i+1}\cdot 1 +\frac {51}{52}\cdot 0\right)=\sum_{i=1}^{52}\frac 1i \approx 4.53&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac 1{n!}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 24 Sep 2017 08:29:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-4.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-4.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
      <item>
        <title>Tutorial 3</title>
        <description>&lt;ol&gt;
  &lt;li&gt;The prover generates an isomorphic graph to a given graph $H$ wherein he knows the Hamiltonian cycles of $G$ and hence for $H$ too. Multiple rounds of questioning either the isomorphism from $G$ to $H$ or hamiltonian cycle in $H$ ensures that the probability of fooling decreases exponentially.&lt;/li&gt;
  &lt;li&gt;The prover provides the coloring for nodes on a particular edge asked by the verifier. For a valid coloring any such edge must be also valid hence ensuring that the prover cannot fool the verifier.&lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n}\left(\frac12\right)^{2i}=\frac 14\frac{1-(1/2)^{2n}}{1-(1/2)^2}\sim \frac 13&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n}\left(\frac12\right)^{3i}=\frac18\frac{1-(1/2)^{3n}}{1-(1/2)^3}\sim\frac17&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;That would be same as two valid english sentences with each letter having fixed distance between corresponding letters, but since language is quite complex that would be very small.&lt;/li&gt;
  &lt;li&gt;That would mean that we would have a substitution that transforms one valid english sentence to another, still very small.&lt;/li&gt;
  &lt;li&gt;No, not perfectly secure because messages that are obtained by decoding cipher text using different key characters at $i$ at $n/2+i$ can not be obtained since the key has such a property, hence it is not truly random because some plain texts have greater probability.&lt;/li&gt;
  &lt;li&gt;Total messages reachable are $k!\frac nk=n(k-1)!&amp;lt;n!$&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 24 Sep 2017 08:03:00 +0000</pubDate>
        <link>https://adityagupta1089.github.io/csl471/tutorial-3.html</link>
        <guid isPermaLink="true">https://adityagupta1089.github.io/csl471/tutorial-3.html</guid>
        
        
        <category>CSL471</category>
        
      </item>
    
  </channel>
</rss>
